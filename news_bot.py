import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import os
import re
import time

# Try to import cloudscraper for medias24.com access
try:
    import cloudscraper
    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

french_months = {
    "01": "janvier", "02": "f√©vrier", "03": "mars", "04": "avril",
    "05": "mai", "06": "juin", "07": "juillet", "08": "ao√ªt",
    "09": "septembre", "10": "octobre", "11": "novembre", "12": "d√©cembre"
}

# STRICT keywords - only for direct Casablanca stock market relevance
STRICT_STOCK_KEYWORDS = [
    "bourse de casablanca", "masi", "madex", "cotation", "introduction en bourse", 
    "ipo", "dividende", "r√©sultat financier", "chiffre d'affaires", "capitalisation boursi√®re",
    "volume d'√©change", "s√©ance boursi√®re", "cours de bourse", "action", "titre",
    "soci√©t√© cot√©e", "indice boursier", "carnet d'ordres", "suspension", "reprise"
]

# Listed companies on Casablanca Stock Exchange (major ones)
LISTED_COMPANIES = [
    "attijariwafa bank", "attijariwafa", "bank of africa", "boa", "bmce bank", "bmce",
    "cih bank", "cih", "maroc telecom", "iam", "ocp group", "ocp", "managem",
    "addoha", "lafargeholcim", "cosumar", "lesieur cristal", "bmci", "cr√©dit du maroc",
    "delta holding", "douja prom", "snep", "alliances", "aradei capital", "jet contractors",
    "vicenne", "nexans maroc", "salafin", "wafa assurance", "atlanta", "rebab company",
    "res dar saada", "marsa maroc", "sodep", "auto hall", "colorado", "disway",
    "ennakl", "fenie brossette", "high tech payment systems", "involys", "label vie",
    "microdata", "risma", "sonasid", "taqa morocco", "timar", "unimer"
]

# High importance: IPOs, major corporate actions, index movements
HIGH_IMPORTANCE_KEYWORDS = [
    "introduction en bourse", "ipo", "masi", "madex", "dividende exceptionnel",
    "r√©sultat annuel", "acquisition", "fusion", "scission", "augmentation de capital",
    "rachat d'actions", "opa", "opv", "suspension de cotation"
]

def get_today_articles():
    now = datetime.now()
    
    # Generate date patterns for the last 24 hours only (today and yesterday)
    date_patterns = []
    for days_back in range(2):  # Today and yesterday only (last 24 hours)
        target_date = now - timedelta(days=days_back)
        day = str(target_date.day)
        month = french_months[target_date.strftime("%m")]
        year = str(target_date.year)
        
        # Add various date format patterns
        date_patterns.extend([
            f"{day} {month} {year}",
            f"{day} {month.capitalize()} {year}",
            f"Lundi {day} {month} {year}",
            f"Mardi {day} {month} {year}",
            f"Mercredi {day} {month} {year}",
            f"Jeudi {day} {month} {year}",
            f"Vendredi {day} {month} {year}",
            f"Samedi {day} {month} {year}",
            f"Dimanche {day} {month} {year}",
            f"Lundi {day} {month.capitalize()} {year}",
            f"Mardi {day} {month.capitalize()} {year}",
            f"Mercredi {day} {month.capitalize()} {year}",
            f"Jeudi {day} {month.capitalize()} {year}",
            f"Vendredi {day} {month.capitalize()} {year}",
            f"Samedi {day} {month.capitalize()} {year}",
            f"Dimanche {day} {month.capitalize()} {year}",
        ])

    # Remove duplicates while preserving order
    seen = set()
    unique_patterns = []
    for pattern in date_patterns:
        if pattern not in seen:
            seen.add(pattern)
            unique_patterns.append(pattern)
    
    date_patterns = unique_patterns

    urls = [
        "https://boursenews.ma/articles/actualite",
        "https://boursenews.ma/articles/marches",
        "https://medias24.com/categorie/leboursier/actus/"
    ]
    
    # Improved headers to avoid being blocked, especially for medias24
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    all_articles = []
    print(f"üîç Looking for STRICT Casablanca stock market & IPO articles from last 24 hours...")
    print(f"üìÖ Date patterns: {len(date_patterns)} patterns for last 24 hours")

    for base_url in urls:
        print(f"\nüìä Checking: {base_url}")
        
        # Use cloudscraper for medias24.com, regular requests for others
        if "medias24.com" in base_url:
            if not CLOUDSCRAPER_AVAILABLE:
                print(f"    ‚ö†Ô∏è  CloudScraper not available for medias24.com - skipping")
                continue
            # Will use cloudscraper instead of headers
            use_cloudscraper = True
        else:
            use_cloudscraper = False
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
        
        for page in range(1, 3):  # Check first 2 pages
            try:
                if "medias24.com" in base_url:
                    # Different pagination for medias24
                    url = base_url + (f"?page={page}" if page > 1 else "")
                else:
                    url = base_url + (f"/{page}" if page > 1 else "")
                
                # Use cloudscraper for medias24.com, regular requests for others
                if use_cloudscraper:
                    scraper = cloudscraper.create_scraper()
                    response = scraper.get(url, timeout=30)
                    print(f"    üîß Using CloudScraper for medias24.com")
                else:
                    response = requests.get(url, headers=headers, timeout=15)
                
                if response.status_code == 403 and not use_cloudscraper:
                    print(f"    ‚ö†Ô∏è  Access blocked (403) - trying alternative approach...")
                    # Try with different user agent
                    headers['User-Agent'] = user_agents[1]
                    response = requests.get(url, headers=headers, timeout=15)
                    if response.status_code == 403:
                        print(f"    ‚ùå Still blocked - website has strong anti-bot protection")
                        break
                elif response.status_code != 200:
                    print(f"    ‚ùå HTTP {response.status_code}")
                    if use_cloudscraper:
                        print(f"    ‚ö†Ô∏è  CloudScraper failed - medias24.com has additional protection")
                    break
                    
                soup = BeautifulSoup(response.text, "html.parser")
                
                # Different selectors for different websites
                if "medias24.com" in base_url:
                    # Medias24 uses different HTML structure - try multiple selectors
                    h3_tags = []
                    
                    # Add a small delay to allow dynamic content loading
                    time.sleep(2)
                    
                    # Try various selectors for medias24
                    selectors = [
                        "article h3", "article h2", "div.article h3", "div.article h2",
                        ".post-title h3", ".post-title h2", ".entry-title", 
                        ".article-title", "h3", "h2", "h1"  # fallback
                    ]
                    
                    for selector in selectors:
                        elements = soup.select(selector)
                        if elements:
                            # Filter out loading messages
                            valid_elements = []
                            for element in elements:
                                text = element.get_text(strip=True)
                                if text and "chargement" not in text.lower() and len(text) > 10:
                                    valid_elements.append(element)
                            
                            if valid_elements:
                                h3_tags = valid_elements
                                print(f"    üìç Using selector: {selector} (found {len(valid_elements)} valid elements)")
                                break
                    
                    if not h3_tags:
                        print(f"    ‚ö†Ô∏è  No valid content found - medias24.com may be loading dynamically")
                else:
                    h3_tags = soup.find_all("h3")
                
                page_articles_found = 0
                for h3 in h3_tags:
                    full_text = h3.get_text(strip=True)
                    a = h3.find("a")
                    if not a:
                        # Sometimes the link is in parent or sibling elements
                        parent = h3.find_parent("a")
                        if parent:
                            a = parent
                        else:
                            continue
                    
                    title = a.get_text(strip=True)
                    link = a.get("href", "")
                    
                    if not link:
                        continue
                    
                    # Fix link format for different websites
                    if link.startswith("http"):
                        full_link = link
                    elif "medias24.com" in base_url:
                        full_link = "https://medias24.com" + link
                    else:
                        full_link = "https://boursenews.ma" + link
                    
                    # Check if recent date (last 24 hours)
                    date_found = any(pattern in full_text for pattern in date_patterns)
                    
                    # STRICT check - only direct stock market relevance (works for both sites)
                    is_stock_related, match_reason = is_strict_stock_market_related(title, full_text)
                    
                    if date_found and is_stock_related:
                        importance = get_article_importance(title, full_text)
                        article_info = {
                            'title': title,
                            'link': full_link,
                            'full_text': full_text,
                            'importance': importance,
                            'match_reason': match_reason,
                            'section': base_url.split('/')[-1],
                            'source': 'Medias24' if 'medias24.com' in base_url else 'BourseNews'
                        }
                        all_articles.append(article_info)
                        page_articles_found += 1
                        print(f"      {importance['emoji']} Found: {title[:60]}...")
                        print(f"         üìç Reason: {match_reason}")
                        print(f"         üåê Source: {article_info['source']}")
                
                print(f"    Page {page}: Found {page_articles_found} strict stock market articles")
                
                if page_articles_found == 0 and page > 1:
                    break

            except Exception as e:
                print(f"    Page {page}: Error - {e}")
                break

    # Sort by importance (most important first)
    all_articles.sort(key=lambda x: x['importance']['level'], reverse=True)
    
    print(f"\nüéØ Total STRICT stock market articles found: {len(all_articles)}")
    return all_articles

def is_strict_stock_market_related(title, content):
    """STRICT check - only direct Casablanca stock market and IPO relevance"""
    text_to_check = (title + " " + content).lower()
    
    # Exclude general economic/international news
    exclude_keywords = [
        "inflation", "pib", "croissance √©conomique", "banque centrale", "politique mon√©taire",
        "bourses europ√©ennes", "bourse am√©ricaine", "wall street", "euro stoxx", "s&p",
        "onee", "oncf", "adm", "onda", "ram", "banque mondiale", "fmi"
    ]
    
    for exclude in exclude_keywords:
        if exclude in text_to_check:
            return False, f"Excluded: {exclude}"
    
    # Check for strict stock market keywords
    for keyword in STRICT_STOCK_KEYWORDS:
        if keyword in text_to_check:
            return True, f"Stock keyword: {keyword}"
    
    # Check for listed companies (strict matching)
    for company in LISTED_COMPANIES:
        if company in text_to_check:
            return True, f"Listed company: {company}"
    
    return False, "No direct stock market relevance"

def get_article_importance(title, content):
    """Determine article importance and assign emoji"""
    text_to_check = (title + " " + content).lower()
    
    # High importance: IPOs, major corporate actions, index movements
    for keyword in HIGH_IMPORTANCE_KEYWORDS:
        if keyword in text_to_check:
            return {'level': 3, 'emoji': 'üö®', 'label': 'Tr√®s Important', 'matched': keyword}
    
    # Medium importance: Company financial results, strategic moves
    medium_keywords = ["r√©sultat", "chiffre d'affaires", "b√©n√©fice", "strat√©gie", "partenariat", "investissement"]
    for keyword in medium_keywords:
        if keyword in text_to_check:
            return {'level': 2, 'emoji': 'üìà', 'label': 'Important', 'matched': keyword}
    
    # Normal importance: General company news
    return {'level': 1, 'emoji': 'üìä', 'label': 'Standard', 'matched': 'g√©n√©ral'}

def summarize_articles_with_gemini(articles, api_key=GEMINI_API_KEY):
    """Create ORIGINAL Arabic summaries for stock market articles with duplicate detection"""
    if not articles:
        return "üì≠ ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿÆÿ®ÿßÿ± ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ° ÿßŸÑŸäŸàŸÖ."
    
    # Group articles by company/topic to detect potential duplicates
    companies_mentioned = {}
    for i, article in enumerate(articles):
        title_lower = article['title'].lower()
        for company in LISTED_COMPANIES:
            if company in title_lower:
                if company not in companies_mentioned:
                    companies_mentioned[company] = []
                companies_mentioned[company].append((i, article))
    
    # Prepare articles for Gemini with source info and duplicate detection
    articles_text = []
    for i, article in enumerate(articles, 1):
        articles_text.append(f"{i}. {article['title']}\nÿßŸÑŸÖÿµÿØÿ±: {article['source']}\nÿßŸÑÿ±ÿßÿ®ÿ∑: {article['link']}")
    
    # Add duplicate information to prompt
    duplicate_info = ""
    if companies_mentioned:
        duplicate_info = "\n\nÿ™ŸÜÿ®ŸäŸá ŸÖŸáŸÖ: ÿ•ÿ∞ÿß Ÿàÿ¨ÿØÿ™ ŸÖŸÇÿßŸÑŸäŸÜ ÿ£Ÿà ÿ£ŸÉÿ´ÿ± Ÿäÿ™ÿ≠ÿØÿ´ÿßŸÜ ÿπŸÜ ŸÜŸÅÿ≥ ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿ£Ÿà ÿßŸÑŸÖŸàÿ∂Ÿàÿπÿå Ÿäÿ±ÿ¨Ÿâ ÿØŸÖÿ¨ŸáŸÖÿß ŸÅŸä ŸÖŸÑÿÆÿµ Ÿàÿßÿ≠ÿØ ÿ¥ÿßŸÖŸÑ ÿ®ÿØŸÑÿßŸã ŸÖŸÜ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™."
    
    prompt = f"""ÿ£ŸÜÿ™ ŸÖÿ≠ŸÑŸÑ ŸÖÿßŸÑŸä ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°. ÿ•ŸÑŸäŸÉ {len(articles)} ŸÖŸÇÿßŸÑ ŸÖÿ™ÿπŸÑŸÇ ÿ®ÿßŸÑÿ®Ÿàÿ±ÿµÿ© ÿßŸÑŸäŸàŸÖ:

{chr(10).join(articles_text)}{duplicate_info}

ÿßŸÑÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿßŸÑÿµÿßÿ±ŸÖÿ©:
1. ÿßŸÉÿ™ÿ® ŸÖŸÑÿÆÿµÿßŸã ÿ£ÿµŸÑŸäÿßŸã ŸàŸÅÿ±ŸäÿØÿßŸã ŸÑŸÉŸÑ ŸÖŸÇÿßŸÑ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (ŸÑŸäÿ≥ ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ©)
2. ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ŸÖŸÇÿßŸÑÿßŸÜ ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ÿπŸÜ ŸÜŸÅÿ≥ ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿ£Ÿà ÿßŸÑŸÖŸàÿ∂Ÿàÿπÿå ÿßÿØŸÖÿ¨ŸáŸÖÿß ŸÅŸä ŸÖŸÑÿÆÿµ Ÿàÿßÿ≠ÿØ ÿ¥ÿßŸÖŸÑ
3. ÿ±ŸÉÿ≤ ŸÅŸÇÿ∑ ÿπŸÑŸâ ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ± ÿπŸÑŸâ ÿßŸÑÿ®Ÿàÿ±ÿµÿ© ŸàÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ±ÿßÿ™
4. ÿ¨ŸÖŸÑÿ™ÿßŸÜ ŸÇÿµŸäÿ±ÿ™ÿßŸÜ ŸÉÿ≠ÿØ ÿ£ŸÇÿµŸâ ŸÑŸÉŸÑ ŸÖŸÑÿÆÿµ (ÿ£Ÿà ÿ´ŸÑÿßÿ´ ÿ¨ŸÖŸÑ ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖÿØŸÖŸàÿ¨)
5. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπÿ±ŸÅÿ™ŸÉ ÿßŸÑŸÖÿßŸÑŸäÿ© ŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑŸÖÿ≠ÿ™ŸÖŸÑ
6. ÿßÿ∞ŸÉÿ± ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑŸÖÿ≠ÿ™ŸÖŸÑ ÿπŸÑŸâ ÿ≥ÿπÿ± ÿßŸÑÿ≥ŸáŸÖ ÿ£Ÿà ÿßŸÑŸÖÿ§ÿ¥ÿ±
7. ÿ™ÿ¨ŸÜÿ® ŸÜÿ≥ÿÆ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ - ÿ£ŸÜÿ¥ÿ¶ ÿ™ÿ≠ŸÑŸäŸÑÿßŸã ÿ£ÿµŸÑŸäÿßŸã
8. ÿ±ÿ™ÿ® ÿ≠ÿ≥ÿ® ÿßŸÑÿ£ŸáŸÖŸäÿ© (ÿßŸÑÿ£ŸÉÿ´ÿ± ÿ£ŸáŸÖŸäÿ© ÿ£ŸàŸÑÿßŸã)
9. ŸÉŸÜ ŸÖÿÆÿ™ÿµÿ±ÿßŸã ŸàŸÖŸÅŸäÿØÿßŸã
10. ŸÑÿß ÿ™ÿ∂ÿπ ÿ±Ÿàÿßÿ®ÿ∑ ŸÅŸä ÿßŸÑŸÖŸÑÿÆÿµ - ÿ≥Ÿäÿ™ŸÖ ÿ•ÿ∂ÿßŸÅÿ™Ÿáÿß ÿ™ŸÑŸÇÿßÿ¶ŸäÿßŸã

ÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÑŸÉŸÑ ŸÖŸÑÿÆÿµ:
[ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑÿ™ÿπÿ®Ÿäÿ±Ÿä] **ÿπŸÜŸàÿßŸÜ ŸÇÿµŸäÿ± (50 ÿ≠ÿ±ŸÅ ŸÉÿ≠ÿØ ÿ£ŸÇÿµŸâ)**
ÿ™ÿ≠ŸÑŸäŸÑ ÿ£ÿµŸÑŸä ŸÅŸä ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÉÿ≠ÿØ ÿ£ŸÇÿµŸâ (ÿ£Ÿà ÿ´ŸÑÿßÿ´ ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖÿØŸÖŸàÿ¨).

ÿßÿ®ÿØÿ£ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ®ÿßŸÑŸÖŸÑÿÆÿµÿßÿ™ÿå ÿ®ÿØŸàŸÜ ŸÖŸÇÿØŸÖÿ©."""

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ]
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        if response.status_code != 200:
            print("‚ùå Gemini API error:", response.text)
            return format_articles_fallback(articles)
        
        result = response.json()
        
        if "candidates" not in result or not result["candidates"]:
            print("‚ùå No candidates in Gemini response")
            return format_articles_fallback(articles)
            
        # Add RTL formatting and links to the generated content
        gemini_content = result["candidates"][0]["content"]["parts"][0]["text"]
        
        # Process the content and add sources at the end of each summary
        formatted_content = ""
        lines = gemini_content.split('\n')
        article_index = 0
        current_summary = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if (line.startswith('üö®') or line.startswith('üìà') or line.startswith('üìä')) and '**' in line:
                # This is a title line with emoji
                if current_summary and article_index > 0:
                    # Finish previous article
                    summary_text = ' '.join(current_summary)
                    if not summary_text.endswith('.'):
                        summary_text += '.'
                    # Use invisible character to prevent link preview compression
                    formatted_content += f"‚Äè{summary_text} üì∞ ‚Äè[ÿßŸÑŸÖÿµÿØÿ±]({articles[min(article_index-1, len(articles)-1)]['link']})\n\n"
                    current_summary = []
                
                # Start new article
                formatted_content += f"‚Äè{line}\n"
                article_index += 1
            elif line.startswith('**') and line.endswith('**'):
                # This is a title line without emoji
                if current_summary and article_index > 0:
                    # Finish previous article
                    summary_text = ' '.join(current_summary)
                    if not summary_text.endswith('.'):
                        summary_text += '.'
                    # Use invisible character to prevent link preview compression
                    formatted_content += f"‚Äè{summary_text} üì∞ ‚Äè[ÿßŸÑŸÖÿµÿØÿ±]({articles[min(article_index-1, len(articles)-1)]['link']})\n\n"
                    current_summary = []
                
                # Start new article
                formatted_content += f"‚Äè{line}\n"
                article_index += 1
            else:
                # This is content
                current_summary.append(line)
        
        # Don't forget the last article
        if current_summary and article_index > 0:
            summary_text = ' '.join(current_summary)
            if not summary_text.endswith('.'):
                summary_text += '.'
            # Use invisible character to prevent link preview compression
            formatted_content += f"‚Äè{summary_text} üì∞ ‚Äè[ÿßŸÑŸÖÿµÿØÿ±]({articles[min(article_index-1, len(articles)-1)]['link']})\n\n"
        
        return formatted_content
        
    except Exception as e:
        print(f"‚ùå Gemini error: {e}")
        return format_articles_fallback(articles)

def format_articles_fallback(articles):
    """Fallback formatting if Gemini fails - in Arabic with RTL"""
    if not articles:
        return "üì≠ ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿÆÿ®ÿßÿ± ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ° ÿßŸÑŸäŸàŸÖ."
    
    formatted = f"‚Äèüìà *ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ° - {len(articles)} ÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸäŸàŸÖ*\n\n"
    
    for article in articles:
        title = article['title'][:70] + ('...' if len(article['title']) > 70 else '')
        formatted += f"‚Äè{article['importance']['emoji']} **{title}**\n"
        formatted += f"‚Äèÿ¥ÿ±ŸÉÿ© ŸÖÿØÿ±ÿ¨ÿ© ŸÅŸä ÿßŸÑÿ®Ÿàÿ±ÿµÿ© ŸÖÿπ ÿ™ÿ£ÿ´Ÿäÿ± ŸÖÿ≠ÿ™ŸÖŸÑ ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿπÿßÿ±. üì∞ [ÿßŸÑŸÖÿµÿØÿ±]({article['link']})\n\n"
    
    return formatted

def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "Markdown",  # Enable markdown for formatting
        "disable_web_page_preview": True,  # Disable link previews to prevent compression
        "disable_notification": False  # Keep notifications enabled
    }
    
    try:
        response = requests.post(url, data=payload, timeout=15)
        if response.status_code == 200:
            print("‚úÖ Message sent to Telegram successfully!")
            return True
        else:
            print(f"‚ùå Telegram API error: {response.status_code}")
            print("Response:", response.text)
            
            # If markdown fails, try with HTML parse mode
            if "parse_mode" in str(response.text).lower():
                print("üîÑ Retrying with HTML parse mode...")
                payload["parse_mode"] = "HTML"
                # Convert markdown to HTML for links
                html_text = text.replace("[ÿßŸÑŸÖÿµÿØÿ±]", "<a href='#'>ÿßŸÑŸÖÿµÿØÿ±</a>")
                html_text = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2">\1</a>', html_text)
                payload["text"] = html_text
                
                response = requests.post(url, data=payload, timeout=15)
                if response.status_code == 200:
                    print("‚úÖ Message sent with HTML formatting!")
                    return True
            
            return False
    except Exception as e:
        print(f"‚ùå Error sending to Telegram: {e}")
        return False

if __name__ == "__main__":
    try:
        print("üéØ STRICT CASABLANCA STOCK MARKET & IPO NEWS BOT")
        print("=" * 60)
        
        # Get today's STRICT stock market articles
        articles = get_today_articles()
        
        if not articles:
            message = "üì≠ ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿÆÿ®ÿßÿ± ŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ° ÿßŸÑŸäŸàŸÖ."
            send_to_telegram(message)
            print("üì≠ No strict stock market articles found for today")
        else:
            print(f"\nüìà Processing {len(articles)} strict stock market articles...")
            
            # Create enhanced Arabic summary with Gemini
            summary = summarize_articles_with_gemini(articles)
            
            # Add header only (no footer) with RTL formatting
            today = datetime.now().strftime("%d %B %Y")
            final_message = f"‚ÄèüèõÔ∏è **ÿ®Ÿàÿ±ÿµÿ© ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°** - {today}\n\n{summary}"
            
            # Send to Telegram
            success = send_to_telegram(final_message)
            
            if success:
                print("‚úÖ Arabic stock market summary sent successfully!")
                print(f"üìä Summary included {len(articles)} articles")
                
                # Show importance breakdown
                high_imp = sum(1 for a in articles if a['importance']['level'] == 3)
                med_imp = sum(1 for a in articles if a['importance']['level'] == 2)
                low_imp = sum(1 for a in articles if a['importance']['level'] == 1)
                
                print(f"üö® Tr√®s Important: {high_imp}")
                print(f"üìà Important: {med_imp}")
                print(f"üìä Standard: {low_imp}")
            else:
                print("‚ùå Failed to send message")
            
    except Exception as e:
        error_message = f"‚Äè‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ®Ÿàÿ™: {str(e)}"
        print(error_message)
        send_to_telegram(error_message) 
